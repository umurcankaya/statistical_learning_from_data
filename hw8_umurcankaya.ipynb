{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW8\n",
    "## Gradient descent algorithm\n",
    "Umur Can Kaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an old optimization algorithm that was known as steepest descent, which I believe is a more self explanatory name. I said old compared to the data science. This algorithm is used to find the extremum points of a continious function $f(\\vec{r})$ of any dimensions. The algorithm starts from any given initial point $\\vec{r} = \\vec{r}_0$ and calculate the gradient at that point. As can be guessed from the name, the direction with the steepest slope is choosen by using the gradient and the point is updated by moving in that direction by a given amount $a$: $\\vec{r}_1 = \\vec{r}_0 - a\\vec{\\nabla}f(\\vec{r})$. This process is repeated until the $\\vec{\\nabla}f(\\vec{r})$ becomes very close to zero. The parameter $a$ is also dependent to the gradient (adaptive) in order to prevent the algorithm from overshooting the desired point. Altough this algorithm is very robust for the functions with only one minima, it generally fails when encounter to the functions with many local minima or saddle points. Since it stops when the gradient becomes ~0, it can not decide if it find a local or global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/350px-Gradient_descent.svg.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/350px-Gradient_descent.svg.png\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data science this method is used to minimize the error of a fit, i.e. $\\chi^2$.\n",
    "$$\\chi^2 = \\sum_i (y(\\vec{r}_i)-y_i)^2$$\n",
    "Where $\\vec{r}_i$ is the independent variable, $y_i$ is the dependent variable and $y(\\vec{r}_i)$ is the prediction/fit. $\\chi^2$ depends on the fit parameters and if there are $M$ fit parameters, $\\chi^2$ defines a surface in the $M$ dimensional parameter space. Since the ultimate goal to minimize $\\chi^2$, the minimum point of the $M$ dimensional surface must be found. This is where the gradient descent algorithm enters. It finds the minima of the hypersurface and the coordinates of this point are the best fit parameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
